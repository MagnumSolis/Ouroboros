# ğŸ›¡ï¸ Sahayak - The Vernacular Financial Sentinel

<div align="center">

![Python](https://img.shields.io/badge/Python-3.11+-blue?style=for-the-badge&logo=python)
![Streamlit](https://img.shields.io/badge/Streamlit-1.29+-red?style=for-the-badge&logo=streamlit)
![Qdrant](https://img.shields.io/badge/Qdrant-Vector%20DB-orange?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

**A Multi-Agent System for Financial Inclusion and Fraud Protection**

*Built for the Convolve Hackathon (Pan-IIT AI/ML Hackathon)*

[Getting Started](#-quick-start) â€¢ [Features](#-features) â€¢ [Architecture](#-architecture) â€¢ [API Reference](#-adapters-api-integrations)

</div>

---

## ğŸ“– Overview

**Sahayak** (à¤¸à¤¹à¤¾à¤¯à¤• = Helper) is an intelligent, vernacular-first financial assistant designed to bridge the digital divide for rural and semi-urban users in India. It acts as a protective layer against financial fraud while simplifying complex banking tasks through natural language interaction.

### ğŸ¯ Key Capabilities

| Capability | Description |
|------------|-------------|
| ğŸ›¡ï¸ **Fraud Detection** | Real-time analysis of transaction patterns and communication for scams |
| ğŸ—£ï¸ **Vernacular Support** | Hindi/English voice and text interaction with automatic language detection |
| ğŸ“š **Financial Literacy** | Explains complex schemes (PMJDY, RBI guidelines) in simple terms |
| ğŸ­ **Emotion Detection** | Voice sentiment analysis to detect stress/fear in fraud victims |
| ğŸ§  **Episodic Memory** | Remembers past conversations for contextual responses |
| âœ… **Source Attribution** | Every answer cites official documents (anti-hallucination) |

---

## âœ¨ Features

### ğŸ¤– Multi-Agent System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      USER INPUT (Voice/Text)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ‘ï¸ PERCEPTION AGENT                                           â”‚
â”‚   â€¢ Audio â†’ Text (Deepgram/Whisper)                              â”‚
â”‚   â€¢ Voice Emotion Detection (Wav2Vec2)                           â”‚
â”‚   â€¢ Image OCR (EasyOCR)                                          â”‚
â”‚   â€¢ Language Detection (Hindi/English)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ§  ORCHESTRATOR AGENT                                          â”‚
â”‚   â€¢ Plans task execution                                         â”‚
â”‚   â€¢ Coordinates other agents                                     â”‚
â”‚   â€¢ Aggregates responses                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                       â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                 â–¼       â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ›¡ï¸ FRAUD AGENT  â”‚ â”‚ ğŸ“š RETRIEVAL    â”‚ â”‚ ğŸ” CRITIC AGENT â”‚
â”‚                 â”‚ â”‚    AGENT        â”‚ â”‚                 â”‚
â”‚ â€¢ Pattern Match â”‚ â”‚ â€¢ Vector Search â”‚ â”‚ â€¢ Verify Facts  â”‚
â”‚ â€¢ OTP/UPI Scams â”‚ â”‚ â€¢ RAG Pipeline  â”‚ â”‚ â€¢ Check Sources â”‚
â”‚ â€¢ Urgency Check â”‚ â”‚ â€¢ Knowledge DB  â”‚ â”‚ â€¢ Ensure Safety â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ¤ Voice Features

| Feature | Technology | Status |
|---------|------------|--------|
| Speech-to-Text | Deepgram (primary), Whisper (fallback) | âœ… Active |
| Text-to-Speech | gTTS (Hindi/English) | âœ… Active |
| Voice Emotion | Wav2Vec2 (HuggingFace) | âœ… Active |
| Language Detection | Auto (Hindi/English/Mixed) | âœ… Active |

**Supported Emotions:** `neutral`, `happy`, `angry`, `sad` (expandable to 7-8 emotions)

### ğŸ“Š Dashboard UI

- **ğŸ’¬ Chat Interface** - Voice & text input with TTS responses
- **ğŸ“š Knowledge Hub** - Upload/ingest PDFs, TXT documents
- **ğŸ” Agent Pipeline** - Visualize agent execution traces
- **ğŸ“ˆ Memory Stats** - View episodic memory and knowledge base counts

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.11+**
- **Git**
- Internet connection for API services

### 1. Clone & Setup

```bash
git clone https://github.com/MagnumSolis/Ouroboros.git
cd Ouroboros/sahayak

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure Environment

```bash
cp .env.example .env
# Edit .env with your API keys
```

### 3. Run Dashboard

```bash
streamlit run dashboard.py
```

Open http://localhost:8501 in your browser.

---

## ğŸ”§ Configuration

### Required API Keys

| Service | Variable | Free Tier | Purpose |
|---------|----------|-----------|---------|
| **Groq** | `GROQ_API_KEY` | 14,400 req/day | Primary LLM (Llama 3.3 70B) |
| **Perplexity** | `PERPLEXITY_API_KEY` | Limited | Online search & verification |
| **Qdrant Cloud** | `QDRANT_URL`, `QDRANT_API_KEY` | 1GB free | Vector database |

### Optional API Keys

| Service | Variable | Purpose |
|---------|----------|---------|
| **Cohere** | `COHERE_API_KEY` | Production embeddings (1024-dim) |
| **Deepgram** | `DEEPGRAM_API_KEY` | Real-time speech-to-text |
| **OpenRouter** | `OPENROUTER_API_KEY` | Backup LLM provider |
| **Gemini** | `GEMINI_API_KEY` | Backup LLM provider |
| **HuggingFace** | `HF_TOKEN` | Faster model downloads |

### Example `.env`

```bash
# LLM Providers
GROQ_API_KEY=gsk_xxx
PERPLEXITY_API_KEY=pplx-xxx

# Vector Database (Qdrant Cloud)
QDRANT_URL=https://xxx.cloud.qdrant.io
QDRANT_API_KEY=xxx

# Embeddings
COHERE_API_KEY=xxx

# Speech
DEEPGRAM_API_KEY=xxx

# HuggingFace (for emotion models)
HF_TOKEN=hf_xxx
```

---

## ğŸ—ï¸ Architecture

### System Overview

```mermaid
graph TB
    subgraph "User Interface"
        UI[Streamlit Dashboard]
        Voice[ğŸ¤ Voice Input]
        Text[âŒ¨ï¸ Text Input]
    end
    
    subgraph "Agent Layer"
        Orch[ğŸ§  Orchestrator]
        Perc[ğŸ‘ï¸ Perception]
        Fraud[ğŸ›¡ï¸ Fraud]
        Ret[ğŸ“š Retrieval]
        Crit[ğŸ” Critic]
    end
    
    subgraph "Adapters"
        LLM[LLM Adapter]
        Embed[Embedding Adapter]
        Speech[Speech Adapter]
        Emotion[Emotion Adapter]
        TTS[TTS Adapter]
    end
    
    subgraph "Memory Layer"
        Episodic[(Episodic Memory)]
        Knowledge[(Knowledge Base)]
        FraudDB[(Fraud Patterns)]
        Cache[(Semantic Cache)]
    end
    
    subgraph "External Services"
        Groq[Groq API]
        Qdrant[Qdrant Cloud]
        Deepgram[Deepgram API]
        Cohere[Cohere API]
    end
    
    UI --> Orch
    Voice --> Perc
    Text --> Orch
    
    Orch --> Fraud
    Orch --> Ret
    Orch --> Crit
    Perc --> Orch
    
    Fraud --> LLM
    Ret --> Embed
    Ret --> Knowledge
    Crit --> LLM
    
    LLM --> Groq
    Embed --> Cohere
    Speech --> Deepgram
    
    Orch --> Episodic
    Fraud --> FraudDB
    LLM --> Cache
```

### Memory Collections (Qdrant)

| Collection | Purpose | Dimension |
|------------|---------|-----------|
| `episodic_memory` | Conversation history & agent traces | 1024 |
| `knowledge_base` | Ingested documents (PMJDY, RBI) | 1024 |
| `fraud_patterns` | Known scam patterns & indicators | 1024 |
| `working_memory` | Active session context | 1024 |
| `semantic_cache` | LLM response caching | 1024 |

---

## ğŸ“ Project Structure

```
sahayak/
â”œâ”€â”€ ğŸ“„ dashboard.py              # Main Streamlit application
â”œâ”€â”€ ğŸ“„ requirements.txt          # Python dependencies
â”œâ”€â”€ ğŸ“„ .env.example              # Environment template
â”‚
â”œâ”€â”€ ğŸ“‚ src/                      # Source code
â”‚   â”œâ”€â”€ ğŸ“‚ adapters/             # External service integrations
â”‚   â”‚   â”œâ”€â”€ llm.py               # LLM providers (Groq, Perplexity, etc.)
â”‚   â”‚   â”œâ”€â”€ embeddings.py        # Embedding providers (Cohere, local)
â”‚   â”‚   â”œâ”€â”€ speech.py            # Speech-to-Text (Deepgram, Whisper)
â”‚   â”‚   â”œâ”€â”€ emotion.py           # Voice emotion detection (Wav2Vec2)
â”‚   â”‚   â”œâ”€â”€ tts.py               # Text-to-Speech (gTTS)
â”‚   â”‚   â”œâ”€â”€ vision.py            # OCR (EasyOCR)
â”‚   â”‚   â””â”€â”€ audio_processor.py   # Unified audio pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ agents/               # Multi-agent system
â”‚   â”‚   â”œâ”€â”€ base.py              # Base agent class
â”‚   â”‚   â”œâ”€â”€ orchestrator.py      # Central coordinator
â”‚   â”‚   â”œâ”€â”€ perception.py        # Multimodal input processing
â”‚   â”‚   â”œâ”€â”€ retrieval.py         # RAG & knowledge retrieval
â”‚   â”‚   â”œâ”€â”€ fraud.py             # Fraud detection
â”‚   â”‚   â””â”€â”€ critic.py            # Response verification
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ memory/               # Memory management
â”‚   â”‚   â”œâ”€â”€ manager.py           # Qdrant interface
â”‚   â”‚   â”œâ”€â”€ collections.py       # Collection definitions
â”‚   â”‚   â”œâ”€â”€ agent_log.py         # Agent execution logging
â”‚   â”‚   â””â”€â”€ cache.py             # Semantic caching
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ ui/                   # UI components
â”‚       â”œâ”€â”€ knowledge_hub.py     # Document upload interface
â”‚       â””â”€â”€ pipeline_viewer.py   # Agent trace visualization
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                  # Utility scripts
â”‚   â”œâ”€â”€ reset_for_demo.py        # Clear data for demo
â”‚   â”œâ”€â”€ ingest_knowledge.py      # Seed knowledge base
â”‚   â”œâ”€â”€ switch_emotion_model.py  # Toggle emotion models
â”‚   â”œâ”€â”€ demo_agent_pipeline.py   # CLI pipeline demo
â”‚   â”œâ”€â”€ demo_voice_emotion.py    # Test voice emotion
â”‚   â”œâ”€â”€ check_episodic_memory.py # Inspect memory
â”‚   â””â”€â”€ test_*.py                # Various test scripts
â”‚
â””â”€â”€ ğŸ“‚ data/                     # Data storage
    â”œâ”€â”€ ğŸ“‚ uploads/              # User uploaded files
    â”œâ”€â”€ ğŸ“‚ knowledge_base/       # Seed documents
    â””â”€â”€ ğŸ“‚ test_docs/            # Sample test documents
```

---

## ğŸ”Œ Adapters (API Integrations)

### LLM Adapter (`src/adapters/llm.py`)

Supports multiple providers with automatic fallback:

| Provider | Model | Use Case |
|----------|-------|----------|
| **Groq** | llama-3.3-70b-versatile | Primary (fastest) |
| **Perplexity** | sonar | Online search |
| **OpenRouter** | gemma-3-12b-it:free | Backup (free) |
| **Gemini** | gemini-2.0-flash | Backup |

### Embedding Adapter (`src/adapters/embeddings.py`)

| Provider | Dimension | Quality |
|----------|-----------|---------|
| **Cohere** | 1024 | Best for multilingual |
| **Sentence Transformers** | 384 | Local fallback |

### Speech Adapter (`src/adapters/speech.py`)

| Provider | Languages | Features |
|----------|-----------|----------|
| **Deepgram** | Hindi, English | Real-time, smart formatting |
| **Whisper** | Multi | Local fallback |

### Emotion Adapter (`src/adapters/emotion.py`)

Pre-trained Wav2Vec2 models for voice emotion:

| Model | Emotions | Size |
|-------|----------|------|
| `superb/wav2vec2-base-superb-er` | 4 | 378MB â­ Default |
| `ehcalabres/wav2vec2-lg-xlsr-en` | 7 | 1.27GB |

Switch models: `python scripts/switch_emotion_model.py`

---

## ğŸ“œ Scripts Reference

| Script | Purpose | Usage |
|--------|---------|-------|
| `reset_for_demo.py` | Clear uploads & Qdrant collections | `python scripts/reset_for_demo.py` |
| `ingest_knowledge.py` | Seed knowledge base with documents | `python scripts/ingest_knowledge.py` |
| `switch_emotion_model.py` | Toggle between emotion models | `python scripts/switch_emotion_model.py` |
| `demo_agent_pipeline.py` | CLI demo of agent pipeline | `python scripts/demo_agent_pipeline.py` |
| `demo_voice_emotion.py` | Test voice emotion detection | `python scripts/demo_voice_emotion.py audio.wav` |
| `check_episodic_memory.py` | Inspect episodic memory entries | `python scripts/check_episodic_memory.py` |
| `setup_qdrant.py` | Initialize Qdrant collections | `python scripts/setup_qdrant.py` |
| `test_adapters.py` | Verify API connections | `python scripts/test_adapters.py` |

---

## ğŸ® Demo Guide

### 1. Fraud Detection Demo

```
User: "Someone called saying my account is blocked and asked for OTP"
```

**Expected Response:**
- ğŸš¨ Fraud alert with severity score
- Explanation of the scam pattern
- Official RBI guidelines quoted
- Action recommendations

### 2. Knowledge Retrieval Demo

```
User: "What is PMJDY and am I eligible?"
```

**Expected Response:**
- Accurate information from ingested documents
- Source citations
- Eligibility criteria explained

### 3. Voice Emotion Demo

1. Record audio expressing stress/fear
2. System detects emotion (angry/fear/sad)
3. Response prioritized based on sentiment
4. Agent pipeline shows detected emotion

---

## ğŸ”§ Troubleshooting

| Issue | Solution |
|-------|----------|
| `Collection doesn't exist` | Run `python scripts/setup_qdrant.py` |
| `python-dotenv parsing error` | Check `.env` for syntax errors (no spaces around `=`) |
| Slow model download | Add `HF_TOKEN` to `.env` |
| Voice emotion not working | Run `pip install transformers torch torchaudio` |
| Dashboard not loading | Check terminal for errors, verify API keys |

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License.

---

<div align="center">

**Built with â¤ï¸ for Financial Inclusion**

*Sahayak - Your Trusted Financial Guardian*

</div>
